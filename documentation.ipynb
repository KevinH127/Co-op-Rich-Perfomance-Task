{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Co-op Rich Perfomance Task\n",
        "#### At U+Education I had to make a Web Crawling Application. Using Python, I had to research libraries and come up with innovative ways to make this application. This is my documentation of my thought process throughout making the application, and also an explaination on how to make it. This documentation also demonstrates what I have learned through this project. What I learned include, learning how to use Selenium, Google APIs, and creating functions in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "#### The goal of this application is to scrape information off the web automatically, minimizing human interaction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1\n",
        "#### First, we need to find and import libraries that can, 1. Open a browser, 2. Parse the HTML code and 3. Read the HTML code.\n",
        "> I did some research and found the best library to work with for me was Selenium. Selenium would allow me to use a browser through Python, parse the HTML code from the browser, and also read the HTML code and turn the elements into text. Therefore, in the code, I imported Selenium's Chrome Browser and it's \"By\" function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver import Chrome\n",
        "from selenium.webdriver.common.by import By"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2\n",
        "#### Initializing AI to help us easily find the text in HTML code\n",
        "> We will be using Google's Gemeni Generative AI API to help us. First we will need to import the API, and create a function so we can talk to the Generative AI. We will use this later on to ask it to find specific text in HTML code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the API\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Initialize the Generative AI\n",
        "API_KEY = 'AIzaSyBrIPsq4D7Z6nDy_XqsIu68WyfegDGXr9E'\n",
        "genai.configure(api_key=API_KEY)\n",
        "geminiModel = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "# Create a function to talk to the AI\n",
        "def gemeni_response(message):\n",
        "  return geminiModel.generate_content(message).text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3\n",
        "#### We then need to initialize variables for the Browser, our URL, and open the Chrome browser.\n",
        "> For the browser, we can initialize a variable called \"driver\" with the Chrome() function. For this application, we will scrape information from the following link, https://westernu.campuslabs.ca/engage/organizations, and scrape the names and emails of all the organizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize variables\n",
        "driver = Chrome()\n",
        "URL = 'https://westernu.campuslabs.ca/engage/organizations'\n",
        "\n",
        "# Open the browser\n",
        "driver.get(URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4\n",
        "#### We must load all the data of the site\n",
        "> When opening the site, you will notice that there is a \"load more\" button. We must use code to press this button however amount of times until all organizaitons are loaded onto the page. We can do this by locating the \"load more\" element and clicing it using Selenium's functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import nessesary libraries (in this case we need a function to pause the program for half a second)\n",
        "from time import sleep\n",
        "\n",
        "# Load All Contents Within the Site\n",
        "while True: \n",
        "  try: \n",
        "    driver.find_element(By.CSS_SELECTOR, 'button[tabindex=\"0\"][type=\"button\"]').click(), sleep(0.05)\n",
        "  except: \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 and 6\n",
        "#### The next step is to scrape the HTML code, and certain elements off the website by using Selenium's \"By\" functions. After finding these elements, we also need to save the information into a JSON file for easy access. \n",
        "> For step 5, we need to know what elements we are looking for and the HTML tags for these elements. We can find this using the developer tools on the browser and selecting the elements you are looking for. For this website, we are specifically looking for the Organization Name, and the Organization Email. In this step, we will also utilize the Generative AI we first intialized and ask us to find the Organization Name and Email in the HTML code.\n",
        "\n",
        "> For step 6, we will need to save our information into a JSON file named, \"westernorgsinfo.json\". Before this, we will need to first load the JSON file, and give the code permission to write in it. We will then save every name and email of the organizations into the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ==================STEP 5==================\n",
        "# Finds all the links to each organization\n",
        "clubs = driver.find_elements(By.CSS_SELECTOR, 'a[href][style=\"display: block; text-decoration: none; margin-bottom: 20px;\"]')\n",
        "\n",
        "# Create a loop to click through all the links found above\n",
        "for i in range(9):\n",
        "  clubs = driver.find_elements(By.CSS_SELECTOR, 'a[href][style=\"display: block; text-decoration: none; margin-bottom: 20px;\"]')\n",
        "  # Click link\n",
        "  clubs[i].click()\n",
        "  sleep(1.25)\n",
        "  \n",
        "  # Get Name of Club\n",
        "  try: \n",
        "    name = driver.find_element(By.CSS_SELECTOR, 'h1[style=\"padding: 13px 0px 0px 85px;\"]').get_attribute('innerHTML')\n",
        "    # Use Generative AI to get Email from HTML Code\n",
        "    name1 = gemeni_response(f'What is this name found in this code (respond with only the name): {name}')\n",
        "  except: \n",
        "    # If no name is found, put n/a\n",
        "    name = 'n/a'\n",
        "\n",
        "  # Get Email of Club\n",
        "  try: \n",
        "    email_card = driver.find_element(By.CSS_SELECTOR, 'div[style=\"margin-left: 5px; padding: 5px 15px; border-left: 1px solid rgb(210, 210, 210);\"]').get_attribute('innerHTML')\n",
        "    # Use Generative AI to get Email from HTML Code\n",
        "    email = gemeni_response(f'What is this email found in this code (respond with only the email): {email_card}')\n",
        "  except: \n",
        "    # If no email was found, put n/a\n",
        "    email = 'n/a'\n",
        "\n",
        "# ==================STEP 6==================\n",
        "  # Import JSON library\n",
        "  import json\n",
        "\n",
        "  # Save info\n",
        "  data = {\n",
        "    'club' : name,\n",
        "    'email' : email\n",
        "  }\n",
        "\n",
        "  # Load JSON file\n",
        "  with open('westernorgsinfo.json', 'a') as JSONFile:\n",
        "    json.dump(data, JSONFile)\n",
        "    # Write info into JSON file\n",
        "    JSONFile.write(',\\n')\n",
        "\n",
        "  # Tells browser to go back to last page\n",
        "  driver.back(), sleep(0.75)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Step\n",
        "#### Check results and see if they are accurate\n",
        "> We can check our results by clicking on the JSON file, \"westernorgsinfo.json\", and seeing if there are 229 organizations recorded. After clicking, you will see that there are 229 organizations, and most if not all information is recorded properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "#### This is how I created my first Web Crawling Application. After some research, and reading Python documentation, I have learned, firstly, how to use Selenium to use Python to search the web, secondly, how to use Google's Generative AI API in my Python applications, and lastly, how to save data from this Python file into another file. Overall, I have succesfully fulfilled my goal of automating finding information on the web, and minimize as much human interation as possible.\n",
        "> Click on the \"demonstration.mp4\" file to look at how this application works."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
